{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03052e65-ca61-4775-859d-cc6decc97bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import string\n",
    "from abc import ABC\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, cast\n",
    "\n",
    "import faiss\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Lemma, Synset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tklearn.config import config\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def get_wordnet_pos(pos_tag: str):\n",
    "    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if pos_tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif pos_tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    elif pos_tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif pos_tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_context_window(\n",
    "    tokens: list[str], target_word: str, window_size: int = -1\n",
    ") -> List[str]:\n",
    "    \"\"\"Get words around the target word within a fixed window size\"\"\"\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    if window_size < 0:\n",
    "        return tokens\n",
    "    try:\n",
    "        target_idx = tokens.index(target_word.lower())\n",
    "        start = max(0, target_idx - window_size)\n",
    "        end = min(len(tokens), target_idx + window_size + 1)\n",
    "        return tokens[start:end]\n",
    "    except ValueError:\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def get_related_words(synset: Synset, depth: int = 2) -> List[str]:\n",
    "    related: List[str] = []\n",
    "    # Add words from definition\n",
    "    related.extend(word_tokenize(synset.definition()))\n",
    "    # Add example sentences\n",
    "    for example in synset.examples():\n",
    "        related.extend(word_tokenize(example))\n",
    "    # Add lemma names\n",
    "    lemmas_: List[Lemma] = synset.lemmas()\n",
    "    related.extend(sum([word_tokenize(lemma.name()) for lemma in lemmas_], []))\n",
    "    # Add hypernyms and hyponyms up to specified depth\n",
    "    if depth > 0:\n",
    "        for hypernym in synset.hypernyms():\n",
    "            related.extend(get_related_words(hypernym, depth - 1))\n",
    "        for hyponym in synset.hyponyms():\n",
    "            related.extend(get_related_words(hyponym, depth - 1))\n",
    "    # Clean and normalize\n",
    "    related = [\n",
    "        word.lower()\n",
    "        for word in related\n",
    "        if word.lower() not in stopwords.words(\"english\")\n",
    "        and word not in string.punctuation\n",
    "    ]\n",
    "    return related\n",
    "\n",
    "\n",
    "def compute_score_by_overlap(\n",
    "    context_words: Iterable[str], sense_words: Iterable[str]\n",
    ") -> int:\n",
    "    return len(set(context_words).intersection(sense_words))\n",
    "\n",
    "\n",
    "def lesk_wsd(args: dict | pd.Series) -> List[dict]:\n",
    "    if isinstance(args, pd.Series):\n",
    "        args = args.to_dict()\n",
    "    tokens: List[str] = args[\"tokens\"]\n",
    "    target_word: str = args[\"token\"]\n",
    "    pos: str | None = args.get(\"pos_tag\")\n",
    "    # Get context window around target word\n",
    "    context_words = get_context_window(tokens, target_word)\n",
    "    # If POS is not provided, try to determine it\n",
    "    if pos is None:\n",
    "        tagged = pos_tag([target_word])[0][1]\n",
    "        pos = get_wordnet_pos(tagged)\n",
    "    # Get all possible synsets for the target word\n",
    "    candidate_synsets: List[Synset]\n",
    "    if pos:\n",
    "        candidate_synsets = wn.synsets(target_word, pos)\n",
    "    else:\n",
    "        candidate_synsets = wn.synsets(target_word)\n",
    "    if not candidate_synsets:\n",
    "        return []\n",
    "    # Find the synset with the highest overlap with context\n",
    "    candicates = []\n",
    "    for sense in candidate_synsets:\n",
    "        # Get related words for this sense\n",
    "        sense_words = get_related_words(sense)\n",
    "        # Compute overlap between context and sense words\n",
    "        score = compute_score_by_overlap(context_words, sense_words)\n",
    "        sense_name = sense.name()\n",
    "        sense_def = sense.definition()\n",
    "        candicates.append({\n",
    "            \"synset\": sense_name,\n",
    "            \"sense_def\": sense_def,\n",
    "            \"score\": score,\n",
    "        })\n",
    "    return candicates\n",
    "\n",
    "\n",
    "def get_wordnet_examples() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve all examples from WordNet with their associated words/phrases\"\"\"\n",
    "    examples = []\n",
    "    # Iterate through all synsets in WordNet\n",
    "    all_sunsets: List[Synset] = list(wn.all_synsets())\n",
    "    for synset in list(all_sunsets):\n",
    "        # Get examples for the synset\n",
    "        for example in synset.examples():\n",
    "            # Get all lemma names (words) associated with this synset\n",
    "            for lemma in synset.lemmas():\n",
    "                lemma = cast(Lemma, lemma)\n",
    "                lemma_str = lemma.name()\n",
    "                # check lemma in example\n",
    "                if lemma_str in example:\n",
    "                    examples.append({\n",
    "                        \"lemma\": lemma_str,\n",
    "                        \"example\": example,\n",
    "                        \"pos\": synset.pos(),\n",
    "                        \"synset\": synset.name(),\n",
    "                        \"definition\": synset.definition(),\n",
    "                    })\n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "\n",
    "def create_prompt(\n",
    "    row: dict = None, /, *, lemma: str = None, example: str = None\n",
    ") -> str:\n",
    "    \"\"\"Create a prompt in the specified format\"\"\"\n",
    "    lemma = row[\"lemma\"] if lemma is None else lemma\n",
    "    example = row[\"example\"] if example is None else example\n",
    "    return f'Given the word \"{lemma}\", here is an example of its usage: \"{example}\"'\n",
    "\n",
    "\n",
    "def create_prompts(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Create prompts in the specified format\"\"\"\n",
    "    return df.apply(create_prompt, axis=1).tolist()\n",
    "\n",
    "\n",
    "def iter_generate_embeddings(\n",
    "    texts: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    ") -> Iterable[np.ndarray]:\n",
    "    \"\"\"Generate embeddings for a list of texts using sentence-transformers\"\"\"\n",
    "    # Generate embeddings\n",
    "    # Process in batches to manage memory\n",
    "    kwargs = {\"desc\": \"Generating embeddings\"}\n",
    "    for i in tqdm(range(0, len(texts), batch_size), **kwargs):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_tensor=True)\n",
    "        yield batch_embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "def generate_embeddings(\n",
    "    texts: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a list of texts using sentence-transformers\"\"\"\n",
    "    # Generate embeddings\n",
    "    embeddings = []\n",
    "    for batch_embeddings in iter_generate_embeddings(\n",
    "        texts, model=model, batch_size=batch_size\n",
    "    ):\n",
    "        embeddings.append(batch_embeddings)\n",
    "    # Concatenate all embeddings\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def create_vector_db(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    # get dimensionality of embeddings\n",
    "    d = embeddings.shape[1]\n",
    "    # create FAISS index\n",
    "    # using L2 (Euclidean) distance for similarity search\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    # add vectors to the index\n",
    "    index.add(embeddings.astype(np.float32))  # FAISS requires float32\n",
    "    # return index\n",
    "    return index\n",
    "\n",
    "\n",
    "class WordSenseDisambiguator(ABC):\n",
    "    def __init__(self, *, cache_dir: str | None = None):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.tokenize = word_tokenize\n",
    "        self.pos_tag = pos_tag\n",
    "        self.batch_size = 32\n",
    "        self.embedding_model_name_or_path = (\n",
    "            \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        )\n",
    "        self.k = 5\n",
    "        self.cache_dir = (\n",
    "            Path(cache_dir) if cache_dir else Path(config.cache_dir) / \"wsd\"\n",
    "        )\n",
    "        self.__post_init__()\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # cache_dir\n",
    "        model = SentenceTransformer(self.embedding_model_name_or_path)\n",
    "        # Get examples and create prompts\n",
    "        df = get_wordnet_examples()\n",
    "        vector_index_path = self.cache_dir / \"vector_index-v0.2.faiss\"\n",
    "        # create parent folder if not exists\n",
    "        vector_index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if vector_index_path.exists():\n",
    "            self.vector_index = faiss.read_index(str(vector_index_path))\n",
    "        else:\n",
    "            prompts = create_prompts(df)\n",
    "            # Generate embeddings\n",
    "            embeddings = generate_embeddings(\n",
    "                prompts, model, batch_size=self.batch_size\n",
    "            )\n",
    "            # Create FAISS index\n",
    "            self.vector_index = create_vector_db(embeddings)\n",
    "            faiss.write_index(self.vector_index, str(vector_index_path))\n",
    "        self.embedding_model = model\n",
    "        self.wordnet_examples_df = df\n",
    "\n",
    "    def find_candidates(self, df: pd.DataFrame) -> List[List[str]]:\n",
    "        candicates = []\n",
    "        prompts = df.apply(\n",
    "            lambda row: create_prompt(lemma=row[\"token\"], example=row[\"text\"]),\n",
    "            axis=1,\n",
    "        ).tolist()\n",
    "        for batch_embeddings in iter_generate_embeddings(\n",
    "            prompts,\n",
    "            self.embedding_model,\n",
    "            batch_size=self.batch_size,\n",
    "        ):\n",
    "            # D[i, j] contains the distance from the i-th\n",
    "            #   query vector to its j-th nearest neighbor.\n",
    "            # I[i, j] contains the id of the j-th nearest\n",
    "            #   neighbor of the i-th query vector.\n",
    "            D, I = self.vector_index.search(batch_embeddings, self.k)\n",
    "            for i in range(len(batch_embeddings)):\n",
    "                item_candicates = []\n",
    "                for j in range(self.k):\n",
    "                    distance = D[i, j]\n",
    "                    neighbor_id = I[i, j]\n",
    "                    neighbor_row = self.wordnet_examples_df.iloc[neighbor_id]\n",
    "                    synset = neighbor_row[\"synset\"]\n",
    "                    definition = neighbor_row[\"definition\"]\n",
    "                    item_candicates.append({\n",
    "                        \"distance\": distance,\n",
    "                        \"synset\": synset,\n",
    "                        \"definition\": definition,\n",
    "                    })\n",
    "                candicates.append(item_candicates)\n",
    "        return candicates\n",
    "\n",
    "    def disambiguate(self, text: str | List[str] | pd.Series) -> List[str]:\n",
    "        if isinstance(text, str):\n",
    "            return self.disambiguate([text])\n",
    "        df = pd.DataFrame(text, columns=[\"text\"])\n",
    "        df.reset_index(drop=False, names=\"index\", inplace=True)\n",
    "        tokens = df[\"text\"].apply(self.tokenize)\n",
    "        df = df.assign(\n",
    "            tokens=tokens,\n",
    "            token=tokens.apply(\n",
    "                lambda x: [(i, *t) for i, t in enumerate(self.pos_tag(x))]\n",
    "            ),\n",
    "        ).explode(\"token\", ignore_index=True)\n",
    "        df = df.assign(\n",
    "            token_index=df[\"token\"].apply(operator.itemgetter(0)),\n",
    "            token=df[\"token\"].apply(operator.itemgetter(1)),\n",
    "            pos_tag=df[\"token\"]\n",
    "            .apply(operator.itemgetter(2))\n",
    "            .apply(get_wordnet_pos),\n",
    "        )\n",
    "        type_1_cand = self.find_candidates(df)\n",
    "        type_2_cand = df.apply(lesk_wsd, axis=1)\n",
    "        candicates = []\n",
    "        for i in range(len(type_1_cand)):\n",
    "            cand_1 = {item[\"synset\"]: item for item in type_1_cand[i]}\n",
    "            cand_2 = {item[\"synset\"]: item for item in type_2_cand[i]}\n",
    "            item_candicates = []\n",
    "            for key in set(cand_1.keys()).union(cand_2.keys()):\n",
    "                item_candicates.append({\n",
    "                    \"synset\": key,\n",
    "                    \"distance\": cand_1.get(key, {}).get(\"distance\", None),\n",
    "                    \"score\": cand_2.get(key, {}).get(\"score\", None),\n",
    "                    \"definition\": cand_1.get(key, cand_2.get(key, {})).get(\n",
    "                        \"definition\", None\n",
    "                    ),\n",
    "                })\n",
    "            candicates.append(item_candicates)\n",
    "        df = (\n",
    "            df.assign(candicates=candicates)\n",
    "            .explode(\"candicates\", ignore_index=True)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        # .rename(columns=lambda x: f\"candidates.{x}\"),\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                df[\"candicates\"].apply(pd.Series),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449a9cf-5baf-4025-bd1f-372ead74134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd = WordSenseDisambiguator()\n",
    "\n",
    "examples = [\n",
    "    \"The bank was robbed last night.\",\n",
    "    \"I sat by the river bank and read a book.\",\n",
    "    \"The bass line of the song was very catchy.\",\n",
    "    \"The bass singer had a deep voice.\",\n",
    "    \"The bat flew around the room.\",\n",
    "    \"The baseball player swung the bat.\",\n",
    "]\n",
    "df = wsd.disambiguate(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735a5c8-986f-4959-85da-df9222a7190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = disambiguated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9929cc5-a022-4644-aa0c-4a221774ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"token\"] == \"bank\"].head(50).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a02df-802f-4ecd-b8d1-33be87133300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wsd.wordnet_examples_df.head().to_csv(index=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a8250-db33-43b7-90b6-4e973ede4fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c04bf9-228f-4f69-bda1-69081fc278eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import FillMaskPipeline, pipeline\n",
    "\n",
    "\n",
    "def masked_lm_pipeline():\n",
    "    return pipeline(model=\"google-bert/bert-base-uncased\")\n",
    "\n",
    "\n",
    "def create_masked_sentence(sentence: str, target_word: str, masked_token: str):\n",
    "    sentence = re.sub(r\"\\b\" + target_word + r\"\\b\", masked_token, sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def generate_similar_examples(\n",
    "    sentence, lemma, pipeline: FillMaskPipeline, top_k: int = 3\n",
    "):\n",
    "    masked_token = pipeline.tokenizer.mask_token\n",
    "    masked_sentence = create_masked_sentence(sentence, lemma, masked_token)\n",
    "    outputs = pipeline(masked_sentence, top_k=top_k)\n",
    "    new_sentences = []\n",
    "    for token in outputs:\n",
    "        token_score = token[\"score\"]\n",
    "        predicted_word: str = token[\"token_str\"]\n",
    "    return new_sentences\n",
    "\n",
    "\n",
    "def expand_lemmas(df: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:\n",
    "    pipeline = masked_lm_pipeline()\n",
    "    expanded_data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        lemma = row[\"lemma\"]\n",
    "        example = row[\"example\"]\n",
    "        pos = row[\"pos\"]\n",
    "        synset = row[\"synset\"]\n",
    "        new_examples = generate_similar_examples(\n",
    "            example, lemma, pipeline, top_k=top_k\n",
    "        )\n",
    "        expanded_data.append(row)\n",
    "        for new_example in new_examples:\n",
    "            expanded_row = {\n",
    "                \"lemma\": lemma,\n",
    "                \"example\": new_example,\n",
    "                \"pos\": pos,\n",
    "                \"synset\": synset,\n",
    "            }\n",
    "            expanded_data.append(expanded_row)\n",
    "    return pd.DataFrame(expanded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede31fd7-ac9b-4d4a-b385-e83d93b4e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = masked_lm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd84ad8-e19c-41ba-8c14-955d01714907",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_similar_examples(\"this is a masked sentence\", \"masked\", pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882380f-fff3-479f-8b2b-8c9555c33e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
