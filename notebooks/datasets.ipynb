{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fc0dc2-7a1d-44ea-9d5c-a0e8c911ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yasas/Documents/Projects/textkit-learn\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad2ee71-7690-45b7-9328-285327342f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tklearn.datasets import load_dataset\n",
    "\n",
    "from tklearn.utils import cache\n",
    "\n",
    "# cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae9cd4b-e0b9-40a3-8896-6db67f805111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "            .tklearn-container {\n",
       "                border: 1px solid #ccc;\n",
       "                padding: 10px;\n",
       "                margin: 10px;\n",
       "            }\n",
       "            </style>\n",
       "            <div class=\"tklearn-container\">\n",
       "                <table style=\"width: 100%;\">\n",
       "                    <tr>\n",
       "                        <th class=\"label\">Path</th>\n",
       "                        <th>Format</th>\n",
       "                        <th>#Rows</th>\n",
       "                        <th>#Columns</th>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>/Users/yasas/.tklearn/cache/dataset-mapped-f31ad6af507cc043ab4b3b6f48b4d101</td>\n",
       "                        <td>arrow</td>\n",
       "                        <td>276872</td>\n",
       "                        <td>6</td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "                <hr>\n",
       "                <div><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grouping</th>\n",
       "      <th>fold</th>\n",
       "      <th>text</th>\n",
       "      <th>target_groups</th>\n",
       "      <th>dataset</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>RIP, Captain.  My condolences to his wife, fam...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>There's nothing wrong with sex between two con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>sbic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>Ultimately a cis gay man (like me for the reco...</td>\n",
       "      <td>[men, transgender men, transgender women, wome...</td>\n",
       "      <td>kennedy2020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>There most certainly are some words that I dou...</td>\n",
       "      <td>[]</td>\n",
       "      <td>civilcomments</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>race/ethnicity</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; a vo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>hatexplain</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "            </div>"
      ],
      "text/plain": [
       "Dataset(/Users/yasas/.tklearn/cache/dataset-mapped-f31ad6af507cc043ab4b3b6f48b4d101, arrow, 276872)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = load_dataset('json', path='notebooks/data/demographic_category_hate_corpora.jsonl')\n",
    "\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c83fb38-08a8-4c2a-a307-26a9a9fa3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb8ec062-933e-4a43-8db8-c78b34381e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group_count = Counter()\n",
    "target_group_map = {}\n",
    "\n",
    "for row in dset:\n",
    "    for surface_target_group in row['target_groups']:\n",
    "        target_group = surface_target_group.replace('folks', 'people')\n",
    "        target_group = re.sub(r'[^a-zA-Z0-9]', '_', target_group)\n",
    "        target_group = re.sub(r'_+', '_', target_group)\n",
    "        target_group_count.update((target_group,))\n",
    "        if surface_target_group not in target_group_map:\n",
    "            target_group_map[surface_target_group] = target_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da78bcb-93d2-48a0-9576-d441731ea829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e0d208-2647-4bcb-bce6-ee0d250ac846",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_target_groups = pd.DataFrame(target_group_count.most_common(50)).iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5da4566-2c80-49ae-87a2-8db931ae6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    target_group_set = set(doc['target_groups'])\n",
    "    doc_target_groups = dict()\n",
    "    for target_group, cid in target_group_map.items():\n",
    "        if cid in doc_target_groups:\n",
    "            continue\n",
    "        if target_group in target_group_set:\n",
    "            doc_target_groups[cid] = True\n",
    "        elif cid not in doc_target_groups:\n",
    "            doc_target_groups[cid] = False\n",
    "    text_input = doc['text']\n",
    "    return {\n",
    "        'id': hash(text_input),\n",
    "        'text': text_input,\n",
    "        'split': {\n",
    "            'train': doc['fold'] == 'train',\n",
    "            'test': doc['fold'] == 'test',\n",
    "        },\n",
    "        'label': doc['hate'],\n",
    "        'target_group': doc_target_groups,\n",
    "        '_source': doc\n",
    "    }\n",
    "\n",
    "\n",
    "docs = dset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f38f0324-6730-41db-a605-18b4585ee050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc8f122-b82f-400f-83e2-745238beec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stream_order(doc, target_groups: List[str]):\n",
    "    stream_order = 0  # no target groups\n",
    "    all_tgt_grps = set([key for key, value in doc['target_group'].items() if value])\n",
    "    for i, target_group in enumerate(target_groups):\n",
    "        if target_group in all_tgt_grps:\n",
    "            stream_order = max(stream_order, i + 1)\n",
    "            all_tgt_grps = all_tgt_grps - {target_group}\n",
    "    if len(all_tgt_grps) > 0:\n",
    "        stream_order = -1\n",
    "    doc['stream_order'] = stream_order\n",
    "    return doc\n",
    "\n",
    "\n",
    "stream = docs.map(\n",
    "    partial(\n",
    "        extract_stream_order,\n",
    "        target_groups=common_target_groups\n",
    "    ),\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9c4c41-6a54-421b-8d29-18d96f5ac35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae51248-7fab-4985-a3be-bcc159f88929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(lf):\n",
    "    sample_size_expr = (\n",
    "        pl.int_range(500, 2000)\n",
    "        .shuffle()\n",
    "        .first()\n",
    "        .over('stream_order')\n",
    "        .alias('sample_size')\n",
    "    )\n",
    "    random_order_expr = (\n",
    "        pl.int_range(0, pl.count())\n",
    "        .shuffle()\n",
    "        .over('stream_order')\n",
    "        .alias('random')\n",
    "    )\n",
    "    sample_expr = random_order_expr <= sample_size_expr\n",
    "    exclude_no_target_expr = pl.col('stream_order') != 0\n",
    "    lf = lf.filter(sample_expr)\n",
    "    lf = lf.filter(exclude_no_target_expr)\n",
    "    return lf.sort(pl.col('stream_order'))\n",
    "\n",
    "\n",
    "sampled_stream = stream.map(sample, mode='polars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1324b756-4e94-4e86-9b4d-87cfcb7cd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c7a957-a1a5-40f1-9134-1c89ae39ac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "            .tklearn-container {\n",
       "                border: 1px solid #ccc;\n",
       "                padding: 10px;\n",
       "                margin: 10px;\n",
       "            }\n",
       "            </style>\n",
       "            <div class=\"tklearn-container\">\n",
       "                <table style=\"width: 100%;\">\n",
       "                    <tr>\n",
       "                        <th class=\"label\">Path</th>\n",
       "                        <th>Format</th>\n",
       "                        <th>#Rows</th>\n",
       "                        <th>#Columns</th>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td>/Users/yasas/.tklearn/cache/dataset-mapped-a307f7bc92921e2ffd62e6eaa4663c1e</td>\n",
       "                        <td>arrow</td>\n",
       "                        <td>1774</td>\n",
       "                        <td>7</td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "                <hr>\n",
       "                <div><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>target_group</th>\n",
       "      <th>_source</th>\n",
       "      <th>stream_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8767052579124479918</td>\n",
       "      <td>JESUS is\\nTHE WORD \\nTHE WORD\\nof GOD\\nthat cr...</td>\n",
       "      <td>{'test': True, 'train': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>{'_urban_minorities_': False, 'aboriginals': F...</td>\n",
       "      <td>{'dataset': 'sbic', 'fold': 'test', 'grouping'...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5283522515298229097</td>\n",
       "      <td>When I make love with my girlfriend she says r...</td>\n",
       "      <td>{'test': True, 'train': False}</td>\n",
       "      <td>True</td>\n",
       "      <td>{'_urban_minorities_': False, 'aboriginals': F...</td>\n",
       "      <td>{'dataset': 'sbic', 'fold': 'test', 'grouping'...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025225533346006765</td>\n",
       "      <td>What did the Pulse Nightclub shooter say to th...</td>\n",
       "      <td>{'test': True, 'train': False}</td>\n",
       "      <td>True</td>\n",
       "      <td>{'_urban_minorities_': False, 'aboriginals': F...</td>\n",
       "      <td>{'dataset': 'sbic', 'fold': 'test', 'grouping'...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7862002707833869562</td>\n",
       "      <td>What's the difference between a pregnant woman...</td>\n",
       "      <td>{'test': True, 'train': False}</td>\n",
       "      <td>True</td>\n",
       "      <td>{'_urban_minorities_': False, 'aboriginals': F...</td>\n",
       "      <td>{'dataset': 'sbic', 'fold': 'test', 'grouping'...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7629227041142280643</td>\n",
       "      <td>I'm fine with gays, and I'm fine with lesbians...</td>\n",
       "      <td>{'test': True, 'train': False}</td>\n",
       "      <td>True</td>\n",
       "      <td>{'_urban_minorities_': False, 'aboriginals': F...</td>\n",
       "      <td>{'dataset': 'sbic', 'fold': 'test', 'grouping'...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "            </div>"
      ],
      "text/plain": [
       "Dataset(/Users/yasas/.tklearn/cache/dataset-mapped-a307f7bc92921e2ffd62e6eaa4663c1e, arrow, 1774)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_stream.filter(pc.field('stream_order') == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fef18c7-e31d-4e4e-b85b-deb6e18411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_stream.to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03891eed-e959-4e17-994f-fba76e245b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bedbeb-c720-48fd-97e0-49a7c7abd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit as ShuffleSplit\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from tklearn import datasets\n",
    "from tklearn.metrics import TextClassificationMetric\n",
    "from tklearn.nn.trainer import Trainer\n",
    "from tklearn.nn.evaluator import Evaluator\n",
    "from tklearn.nn.callbacks import ProgbarLogger\n",
    "from tklearn.config import config, config_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db99b8ac-768b-4847-bf18-ef2bcc1e5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import auto as tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def evaluate_classes(target_group_test_df, y_score, y_true):\n",
    "    results_table = []\n",
    "    for target_group in tqdm.tqdm(target_group_test_df.columns):\n",
    "        target_group_idx = target_group_test_df[target_group]\n",
    "        target_group_y_score = y_score[target_group_idx]\n",
    "        target_group_y_pred = target_group_y_score >= 0.5\n",
    "        target_group_y_true = y_true[target_group_idx]\n",
    "        if len(target_group_y_true) == 0:\n",
    "            continue\n",
    "        if len(target_group_y_true.unique()) != 2:\n",
    "            continue\n",
    "        auc = roc_auc_score(target_group_y_true, target_group_y_score)\n",
    "        num_samples = target_group_y_true.sum()\n",
    "        results_table.append([target_group, num_samples, auc])\n",
    "    return pd.DataFrame(\n",
    "        results_table, columns=['target_group', 'num_samples', 'auc']\n",
    "    ).sort_values('num_samples', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d19a2a2-4844-453a-a659-5d3df3cbe69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return pd.Series(tokenizer(s, padding=\"max_length\", truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ba5cac9-06f5-43c2-a259-eab8ff68609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3241b547-58bf-4833-b28c-65b3d776c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99398e0274b544ed91cc4940b39f854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caac2200c7c4c83ad428d09f38fe256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[ProgbarLogger()])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m     30\u001b[0m test_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream_order\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m step)\u001b[38;5;241m.\u001b[39mfilter(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstruct\u001b[38;5;241m.\u001b[39mfield(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/tklearn/nn/trainer.py:312\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, x, y, evaluator)\u001b[0m\n\u001b[1;32m    310\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(loss\u001b[38;5;241m=\u001b[39mrunning_loss \u001b[38;5;241m/\u001b[39m num_batches))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(eval_results)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, logs\u001b[38;5;241m=\u001b[39mepoch_logs)\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/tklearn/nn/evaluator.py:120\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    118\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m val_loss\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_name, group_metric \u001b[38;5;129;01min\u001b[39;00m grouped_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 120\u001b[0m     group_metric_results \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group_metric_key, group_metric_value \u001b[38;5;129;01min\u001b[39;00m group_metric_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    122\u001b[0m         result[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_metric_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m group_metric_value\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/tklearn/metrics/base.py:219\u001b[0m, in \u001b[0;36mTextClassificationMetric.result\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MetricOutputType:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tc_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/tklearn/metrics/base.py:94\u001b[0m, in \u001b[0;36mUnionMetric.result\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics:\n\u001b[0;32m---> 94\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m         store \u001b[38;5;241m=\u001b[39m output_dict\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/tklearn/metrics/base.py:178\u001b[0m, in \u001b[0;36mHuggingFaceMetric.result\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     avg_strategy \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    177\u001b[0m out: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# rename key to avoid conflict with other metrics\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         out[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/venv/lib/python3.11/site-packages/evaluate/module.py:433\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/venv/lib/python3.11/site-packages/evaluate/module.py:385\u001b[0m, in \u001b[0;36mEvaluationModule._finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Let's acquire a lock on each node files to be sure they are finished writing\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     file_paths, filelocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_cache_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# Read the predictions and references\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/textkit-learn/venv/lib/python3.11/site-packages/evaluate/module.py:302\u001b[0m, in \u001b[0;36mEvaluationModule._get_all_cache_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_process \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation module cache file doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist. Please make sure that you call `add` or `add_batch` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least once before calling `compute`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name]\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`."
     ]
    }
   ],
   "source": [
    "max_step = df.select('stream_order').max().collect().item()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    problem_type='multi_label_classification',\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "open('notebooks/logs/hate-speech-detection-lml-output-v1.log', 'w').close()\n",
    "\n",
    "for step in tqdm.trange(1,  max_step + 1, desc='Streaming'):\n",
    "    train_df = df.filter(pl.col('stream_order') == step).filter(pl.col('split').struct.field('train')).collect()\n",
    "    train_df = train_df.to_pandas()\n",
    "    train_df['labels'] = train_df['label'].astype(np.float32)\n",
    "    train_idx, valid_idx = next(ShuffleSplit().split(train_df, train_df.labels))\n",
    "    valid_df = train_df.iloc[valid_idx]\n",
    "    valid_input_df = valid_df['text'].apply(tokenize)\n",
    "    valid_input_df['labels'] = valid_df['labels'].apply(lambda x: [x])\n",
    "    valid_input_df = valid_input_df.reset_index(drop=True)\n",
    "    train_df = train_df.iloc[train_idx]\n",
    "    train_input_df = train_df['text'].apply(tokenize)\n",
    "    train_input_df['labels'] = train_df['labels'].apply(lambda x: [x])\n",
    "    train_input_df = train_input_df.reset_index(drop=True)\n",
    "    groups_df = pl.from_pandas(train_df).select(['target_group']).unnest('target_group').to_pandas()\n",
    "    evaluator = Evaluator(valid_input_df, metric=TextClassificationMetric(num_labels=1), postprocessor='binary', groups=groups_df)\n",
    "    trainer = Trainer(model, batch_size=8, callbacks=[ProgbarLogger()])\n",
    "    del model\n",
    "    trainer.fit(train_input_df, evaluator=evaluator)\n",
    "    # Test\n",
    "    test_df = df.filter(pl.col('stream_order') == step).filter(pl.col('split').struct.field('test')).collect().to_pandas()\n",
    "    test_df['labels'] = test_df['label'].astype(np.float32)\n",
    "    test_input_df = test_df['text'].apply(tokenize)\n",
    "    test_input_df['labels'] = test_df['labels'].apply(lambda x: [x])\n",
    "    test_input_df = test_input_df.reset_index(drop=True)\n",
    "    groups_df = pl.from_pandas(test_df).select(['target_group']).unnest('target_group').to_pandas()\n",
    "    evaluator = Evaluator(test_input_df, metric=TextClassificationMetric(num_labels=1), postprocessor='binary', groups=groups_df)\n",
    "    eval_report = evaluator.evaluate(trainer)\n",
    "    eval_report['step'] = step\n",
    "    with open('notebooks/logs/hate-speech-detection-lml-output-v1.log', 'a') as out:\n",
    "        out.write(json.dumps(eval_report) + '\\n')\n",
    "    model = trainer.model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e06b59-b450-4e82-9657-88fbcba4abb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
